{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db71f1d9",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 30pt\"> ARTIFICIAL NEURAL NETWORKS </h1>\n",
    "<h1 style=\"text-align: center; font-size: 18pt; margin-bottom:50px; font-family:Arial\"> Abubakar Abdulkadir </h1>\n",
    "<img src='images/dl.jpg'width=\"100%\"/><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781ce48",
   "metadata": {},
   "source": [
    "<h1> The Neural Network Architecture</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53039eff",
   "metadata": {},
   "source": [
    "Artificial Neural Network (ANN) or simply Neural networks is a supervised computational model inspired by how the human brain works. It consists of several computational nodes called Neurons interconnected to process and transmit information, similar to the way neurons in the human brain communicate with each other. Each neuron performs computation, the simplest being a logistic regression, and forwards the output to other neurons in the network. Through this structure, the neural network is able to leevrage on the power of combining several units of basic processing into a bigger, stronger and more accurate system.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad9e53f",
   "metadata": {},
   "source": [
    "<img src='images/nn.PNG' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d802aca6",
   "metadata": {},
   "source": [
    "Before we look at the architecture of a Neural Network, let's build intuition. Assuming we have the task of constructing a bridge. Such a nature of the task will require several specialties like architectural engineering, civil engineers, material scientists, etc. But we only have unskilled labor, meaning we must have to provide training for the workforce. Since there will be training anyway, we have three options: we can choose to pick one of the laborers and attempt to train them in all the required specialties, or we can choose to train all the workforce in all of the required specialties, and lastly, we can choose to divide the workforce into distinct specialties and have each group focus on a specific assignment. Which of these approaches will likely yield the optimal quality of the bridge?\n",
    "\n",
    "Yes, you are correct. If we choose to go with option one, it would be almost impossible for that one person to pay attention to all the details of the specialties and not be overwhelmed. So this approach is not guaranteed to yield the best possible result. If we choose to stick with option two, then we will end up training a larger number of persons who are most likely going to be overwhelmed themselves and, in the end, not be able to grasp the details. With the last option, each person is learning the bits they need to deliver their task in the project to the best possible quality. Hence, approach three will most likely perform better. Approach number three is an analogy of the Artificial Neural Network. This is why Artificial Neural Network shines, especially when we have a large dataset to learn from.\n",
    "\n",
    "Lets look at each component of the arttificial nueral network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3609ba73",
   "metadata": {},
   "source": [
    "### Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f09d8c",
   "metadata": {},
   "source": [
    "Neurons serve as the fundamental building blocks of a neural network. Drawing from the analogy, we can liken a neuron to a single workman, visualized as circles in Figure 1. Each neuron functions as an independent entity that receives input data, performs internal computations, and produces an output. It's worth noting that when the network consists of only one neuron, it is akin to conducting a linear or logistic regression. Figure 1 shows six neurons splitted into three layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e47aaf",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af97e72",
   "metadata": {},
   "source": [
    "Neurons in a neural network are organized into layers. Each layer consists of one or more neurons performing the same task on a set of inputs. The output of one layer becomes the input for the next, enabling information flow and learning.\n",
    "\n",
    "In the analogy, layers represent specialization. For example, in Layer 1, which represents architectural design, neurons specialize in architecture. They learn from the same input and produce architectural outputs. These outputs then become input for Layer 2, representing civil engineering.\n",
    "\n",
    "Layers in a neural network mimic the expertise stages. Specialized teams handle different aspects in real projects. Collaboration between layers enables complex information processing and comprehensive outputs.Dividing the network into layers capitalizes on specialization, fostering collective intelligence that surpasses isolated neurons or layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc27ee6",
   "metadata": {},
   "source": [
    "- Input Layer: Layer 0 is often refered to as the input layer. No computation occurs in this layer and it is not often counted among the layers. It is the layer which holds the dataset. Figure 1 has input layer with 4 features.\n",
    "\n",
    "- Hidden Layer: All layers between the input and the output layer are refered to as hidden layers. Figure 1 has two hidden layers with 3 and 2 neuron respectively.\n",
    "\n",
    "- Output Layer: This is the layer which produces the final result. Figure 1 has output layer with 1 neuron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48e393",
   "metadata": {},
   "source": [
    "<h1> How the Neural Network learns - Intuition </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7144198",
   "metadata": {},
   "source": [
    "Let's analyze the functioning of a neural network using Layer 1 and the analogy of bridge-building process. The input layer contains a dataset of several successful bridges built before. This dataset from layer 0 is passed into layer 1 as training materials. The neurons in layer 1 (the training architechs) looks at the dataset and make thier own sketches using thier current knowledge. Because each nueron in the layer have distinct current knowledge, thier outputs will differ. They will each compare thier own sketch with the dataset of the working images and calculate their level of errors (what doesnt work). Then they will each update thier knowledge accordingly. \n",
    "\n",
    "Although, instead of passing the working images to the next layer (Layer 2), they will pass thier own sketch to avoid layer 2 learning the same thing they have learnt. Now layer 2 will commence their own learning using the sketch from Layer 1, then thier output goes to the output layer. This process is repeated for a large trainner-defined amount of time and with each cycle, nuerons in a layer update thier knowledge until thier errors is as close to zero as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cedca7",
   "metadata": {},
   "source": [
    "<h1> How the Neural Network learns - Technical </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cdffe3",
   "metadata": {},
   "source": [
    "Lets look for a simple task. Assuming we want to predict if a water will boil or not given for parameters such as temperature, level of impurity, atm pressure, amount of water and time. We have the dataset as shown below (note that this is a syntetic dataset just for example). <br><br>\n",
    "<img src='images/dataset_dl.PNG' /> <br><br>\n",
    "The nueral network learns in two phases known as the forward and backward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315546aa",
   "metadata": {},
   "source": [
    "#### Forward Propagation\n",
    "In the forward propagation phase, two computations occur.\n",
    "\n",
    "1. Each neuron in the layers makes predictions using outputs from the previous layer as input (x) together with their current knowledge called weights represented by a vector of real numbers w and bias b which is a real number. This phase gives us $\\hat{y}$ by making the following computations\n",
    "\n",
    "<p style=\"text-align:center; font-size: 14pt\">i.  $z = \\sum_{i=1}^{n}w^i \\times x^i + b$. </p> \n",
    "Where n is the number of training examples, x is the input data and b is the bias.\n",
    "\n",
    "This is a linear function. Even if we train thousands or millions of linear equations, it will always result to a linear equation. This will prevent the nueral network from learning diverse structures at distinct layers. It will therefore reduce our equation to a glorified linear regressor. Hence, we need to apply an activation function to switch the shape of our fit. There are several activation functions such as rectiified liniear unit (ReLu), Hyperbolic Tantent (Tanh), Exponential Linear Unit (ELU), Softmax etc, but for this task, we will make use of the sigmoid.\n",
    "\n",
    "<p style=\"text-align:center; font-size: 14pt\"> ii.  $f(z) = \\frac{1}{1 + e^{-x}}$ </p> \n",
    "<p style=\"text-align:center; font-size: 14pt\"> Then the output of each neuron will be $ \\hat{y} = f(z) $ </p> \n",
    "\n",
    "Assuming W = [2, 1.3, 0.4, 1.7, 2] (these numbers are generated randomly. It is just to initialize the weights) where each entry is the weight assigned to each feature and the training set in figure 2. Then neuron A1 in figure 1 will compute the prediction for the first training example $\\hat{y_1}$ as follows\n",
    "\n",
    "$z = 2 \\times 70 + 1.3 \\times 0.54 + 0.4 \\times 12.7 +1.7 \\times 2 + 5 \\times 2 = 159.182$ <br><br>\n",
    "$\\hat{y_1} = \\frac{1}{1 + e^{-159.182}} = 1$\n",
    "\n",
    "Neuron A1 predicts 1, which is correct for the first training example. Lets look at its prediction for the second training example, \n",
    "$z = 2 \\times 45 + 1.3 \\times 0.2 + 0.4 \\times 15.4 +1.7 \\times 1.5 + 2 \\times 7 = 112.97$ <br><br>\n",
    "$\\hat{y_2} = \\frac{1}{1 + e^{-112.97}} = 1$\n",
    "\n",
    "This time, the neuron gets its prediction wrong. Hence, this leads to the second computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2ee5d",
   "metadata": {},
   "source": [
    "2. In the second computation under the forward propagation, the neuron will compute the difference between its prediction with the actual value for each training example. This is called the loss.\n",
    "\n",
    "<p style=\"text-align:center; font-size: 14pt\">  $L(\\hat{y}, y) = - y \\log{\\hat{y}} - (1 - y)log(1 - \\hat{y})$ </p> \n",
    "\n",
    "The average of the loss across the entire training dataset is termed as the cost and is computed as \n",
    "\n",
    "<p style=\"text-align:center; font-size: 14pt\">  $J(w, b) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})]$\n",
    " </p> \n",
    " \n",
    " The major objective of learning, is to reduce the cost function such that it is as close to zero as possible. 0 cost function will mean that there is no difference between the predicted and actual value across the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e8b73f",
   "metadata": {},
   "source": [
    "#### Bacward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e8b28",
   "metadata": {},
   "source": [
    "Before we discuss bacward propagation, we will notice that our problem has taken a new dimension. It is no longer how to predict or classify. It is now how do we reduce a particular function to be as close to zero as possible. This problem is a minimization problem. One common technique for minimization is the gradient descent. In our case we are looking at using gradient descent to minimize the cost function.\n",
    "\n",
    "<img src=\"images/cost_dl.PNG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3089ddf",
   "metadata": {},
   "source": [
    "The cost function if plotted will look something like the figure above. Although, it is not drawn to scale. But the idea is, it will be bowl shaped and having its depth very close to zero. The zero is to find the parameters w which has the cost at the lowest point. The formular for gradient descent is\n",
    "\n",
    "<p style=\"text-align:center; font-size: 14pt\">  $w_i = w_i - \\lambda \\frac{\\partial J(w, b)}{\\partial w}$ <br><br>\n",
    "    $b = b - \\lambda \\frac{\\partial J(w, b)}{\\partial b}$\n",
    " </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6583d5e5",
   "metadata": {},
   "source": [
    "For every iteration of gradient descent, the parameters of the neuron gets updated by subtracting the result of multiplying the learning rate (a trainer-defined vaue) with the partial derivative of the cost function with respect to the each of the features.\n",
    "\n",
    "For an example, when we compute the cost function for w = 8, the result was seven. Lets perform a single step of gradient descent using one variable w.\n",
    "\n",
    "If we draw a tangent line to the curve at point A and compute the slope at A slope. We would find out that the slope at A will be a positive number. Lets say T. Then we can update w by sutracting T multiplied by the learning rate from it. The value of w will decrease bringing it closer to 0.\n",
    "\n",
    "If we use point B as another example, the slope at that point will be negative. Then if we apply the gradient descent formular to update parameter w, we will find out that subtrating a negative value from w will increase it. Therefore it will move closer to zero. Such is how gradient descent is helpful in minimizing the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c055c",
   "metadata": {},
   "source": [
    "In the backward propagation step, the derivatives are computed and the parameters are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efd1de",
   "metadata": {},
   "source": [
    "### Summary of foward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48712cb",
   "metadata": {},
   "source": [
    "In the forward propagation, a neuron computes its predicted value and computes the cost, then in the backward propagation, the neuron computes the gradient of the cost with respect to all the parameters and updates the parameters accordingly. This forward and backward cycle is repeated for as many times as it takes to update the parameters such that the cost is acceptably close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a694f0",
   "metadata": {},
   "source": [
    "<h1> Pseudocode for Neural Network </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489659b0",
   "metadata": {},
   "source": [
    "1. Define the architectue\n",
    "    - specify how many features are in the input layer\n",
    "    - Add layers to the hidden and output layers\n",
    "    - specify the numbers of neuron in each layer and the activation functions\n",
    "    - Define the number of epochs (The number of cycles the algorithm will iterate over the entire dataset)\n",
    "2. Initialize all the parameters with random values\n",
    "3. If the current number of iteration is still less than the epoch then\n",
    "4.   Compute foward and backward propagations through the network\n",
    "5. Else return the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5826889",
   "metadata": {},
   "source": [
    "<h1> Implementing Neural Networks </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee713900",
   "metadata": {},
   "source": [
    "### 1. With Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2fc5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "a451ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, X, y, epoch, lr=10):\n",
    "        x = np.array(X).T\n",
    "        self.layers = [{\n",
    "            'output_shape': x.shape,\n",
    "            'output_data': x,\n",
    "        }]\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr\n",
    "        self.y = y\n",
    "        self.cost = []\n",
    "        \n",
    "    def Layer(self, units=1, activation='sigmoid'):\n",
    "        prev_layer = self.layers[-1]\n",
    "        weights = np.random.uniform(0, 1, (prev_layer['output_shape'][0], units))\n",
    "        b = random.random()\n",
    "    \n",
    "        \n",
    "        new_layer = {\n",
    "            'output_shape': (units, prev_layer['output_shape'][0]),\n",
    "            'weights': weights,\n",
    "            'b': b,\n",
    "            'output_data': None,\n",
    "            'dw' : None,\n",
    "            'db' : None,\n",
    "            'z': None,\n",
    "            'activation': activation\n",
    "        }\n",
    "        \n",
    "        self.layers.append(new_layer)\n",
    "        \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(z))\n",
    "    \n",
    "    def __relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def __sigmoid_back(self, d_cost, z):\n",
    "        a = 1 / (1 +  np.exp(-z))\n",
    "        dz = d_cost * a * (1 - a)\n",
    "        print(type(dz))\n",
    "        return dz\n",
    "    \n",
    "    def __relu_back(self, cost, z):\n",
    "        dz = np.where(cost <= 0)\n",
    "        dz = np.vstack(dz).T\n",
    "        print(type(dz))\n",
    "        return dz\n",
    "    \n",
    "    def __linear_backward(self, dz, prev_out, w, b):\n",
    "        print(w.shape, dz.shape)\n",
    "        m = prev_out.shape[1]\n",
    "        dW = np.dot(dz, prev_out) / m\n",
    "        db = np.sum(dz, axis=1, keepdims=True) / m\n",
    "        dprev_out = np.dot(w, dz)\n",
    "\n",
    "        return dprev_out, dW, db\n",
    "    \n",
    "    def __adjust_parameters(self):\n",
    "        for layer_index in range(1, len(self.layers)):\n",
    "            cur_layer = self.layers[layer_index]\n",
    "            cur_layer['weights'] = cur_layer['weights'] - (self.lr * cur_layer['dw']).T\n",
    "            cur_layer['b'] = cur_layer['b'] - (self.lr * cur_layer['db'])\n",
    "        \n",
    "    \n",
    "    def __forward_prop(self):\n",
    "        for layer_index in range(1, len(self.layers)):\n",
    "            cur_layer = self.layers[layer_index]\n",
    "            prev_layer = self.layers[layer_index - 1]\n",
    "            \n",
    "            z = np.dot(cur_layer[\"weights\"].T, prev_layer['output_data']) + cur_layer[\"b\"]\n",
    "            \n",
    "            if cur_layer['activation'] == 'sigmoid':\n",
    "                a = self.__sigmoid(z)\n",
    "            elif cur_layer['activation'] == 'relu':\n",
    "                a = self.__relu(z)\n",
    "            \n",
    "            cur_layer[\"output_data\"] = a\n",
    "            cur_layer[\"z\"] = z\n",
    "            \n",
    "        cost = np.sum((a - self.y) ** 2) /  2 * len(a)  \n",
    "        self.cost.append(cost)\n",
    "        \n",
    "    def __backward_prop(self):\n",
    "        for layer_index in range(len(self.layers) - 1, 0, -1):\n",
    "            cur_layer = self.layers[layer_index]\n",
    "            prev_layer = self.layers[layer_index - 1]\n",
    "\n",
    "            if layer_index == len(self.layers) - 1:\n",
    "                cur_d_cost = -(np.divide(self.y, cur_layer['output_data']) \n",
    "                                - np.divide(1 - self.y, 1 \n",
    "                                - cur_layer['output_data']))\n",
    "                \n",
    "            if cur_layer['activation'] == 'sigmoid':\n",
    "                dz =  self.__sigmoid_back(cur_d_cost, cur_layer['z'])\n",
    "            elif cur_layer['activation'] == 'relu':\n",
    "                dz =  self.__relu_back(cur_d_cost, cur_layer['z'])\n",
    "                \n",
    "            cur_d_cost, dW, db = self.__linear_backward(dz, prev_layer['output_data'], cur_layer['weights'], cur_layer['b'])\n",
    "\n",
    "            cur_layer['dw'] = dW\n",
    "            cur_layer['db'] = db\n",
    "             \n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "    def fit(self, verbose=10):\n",
    "        cur_epoch = 0\n",
    "        \n",
    "        while cur_epoch < self.epoch:\n",
    "            self.__forward_prop()\n",
    "            self.__backward_prop()\n",
    "            \n",
    "            if cur_epoch % verbose == 0:\n",
    "                print(self.cost[cur_epoch])\n",
    "                \n",
    "            self.__adjust_parameters()\n",
    "            cur_epoch += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "17be924f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  1  1\n",
       "1  2  3\n",
       "2  3  2\n",
       "3  4  4"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({'a':[1, 2, 3, 4], 'b':[1,3,2,4]})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "2d9477e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNet(data, np.array([1, 0, 0, 1]), epoch=30, lr=0.1)\n",
    "net.Layer(units=4, activation='relu')\n",
    "net.Layer(units=2, activation='relu')\n",
    "net.Layer(units=1, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "f1821ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'output_shape': (2, 4),\n",
       "  'output_data': array([[1, 2, 3, 4],\n",
       "         [1, 3, 2, 4]], dtype=int64)},\n",
       " {'output_shape': (4, 2),\n",
       "  'weights': array([[0.88411299, 0.7940694 , 0.12238534, 0.74736457],\n",
       "         [0.10156638, 0.73964003, 0.37411572, 0.96508838]]),\n",
       "  'b': 0.898072271033767,\n",
       "  'output_data': None,\n",
       "  'dw': None,\n",
       "  'db': None,\n",
       "  'z': None,\n",
       "  'activation': 'relu'},\n",
       " {'output_shape': (2, 4),\n",
       "  'weights': array([[0.9640901 , 0.93924055],\n",
       "         [0.23259076, 0.2000443 ],\n",
       "         [0.59095338, 0.6357712 ],\n",
       "         [0.6479854 , 0.8246112 ]]),\n",
       "  'b': 0.49511078082277593,\n",
       "  'output_data': None,\n",
       "  'dw': None,\n",
       "  'db': None,\n",
       "  'z': None,\n",
       "  'activation': 'relu'},\n",
       " {'output_shape': (1, 2),\n",
       "  'weights': array([[0.79496235],\n",
       "         [0.62397518]]),\n",
       "  'b': 0.7862604086060229,\n",
       "  'output_data': None,\n",
       "  'dw': None,\n",
       "  'db': None,\n",
       "  'z': None,\n",
       "  'activation': 'sigmoid'}]"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "2553467e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2, 1) (1, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,4) and (2,4) not aligned: 4 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [652]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [648]\u001b[0m, in \u001b[0;36mNeuralNet.fit\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cur_epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__forward_prop()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__backward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cur_epoch \u001b[38;5;241m%\u001b[39m verbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost[cur_epoch])\n",
      "Input \u001b[1;32mIn [648]\u001b[0m, in \u001b[0;36mNeuralNet.__backward_prop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cur_layer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     97\u001b[0m     dz \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__relu_back(cur_d_cost, cur_layer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 99\u001b[0m cur_d_cost, dW, db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__linear_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_layer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_layer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_layer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m cur_layer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdw\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dW\n\u001b[0;32m    102\u001b[0m cur_layer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m db\n",
      "Input \u001b[1;32mIn [648]\u001b[0m, in \u001b[0;36mNeuralNet.__linear_backward\u001b[1;34m(self, dz, prev_out, w, b)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(w\u001b[38;5;241m.\u001b[39mshape, dz\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     52\u001b[0m m \u001b[38;5;241m=\u001b[39m prev_out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 53\u001b[0m dW \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_out\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m m\n\u001b[0;32m     54\u001b[0m db \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dz, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m/\u001b[39m m\n\u001b[0;32m     55\u001b[0m dprev_out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(w, dz)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,4) and (2,4) not aligned: 4 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "net.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9fa14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d0b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

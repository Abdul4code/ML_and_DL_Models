{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab57163d",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 30pt\"> DECISION TREES </h1>\n",
    "<img src='images/decision_tree.png' /><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb864e",
   "metadata": {},
   "source": [
    "<h1> What are Decision Trees</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e85a6c",
   "metadata": {},
   "source": [
    "\n",
    "Decision trees are not your typical linear models. They are powerful and versatile flowchart-like models that can be used for both prediction and classification tasks. While linear regression models assume a linear relationship between predictors and targets, decision trees break free from such constraints. They are non-parametric models, allowing them to capture complex nonlinear relationships between predictors and targets.\n",
    "\n",
    "The beauty of decision trees lies in their ability to handle real-world complexities. They have found immense popularity in various industries, making a significant impact in fields such as healthcare, manufacturing, banking and finance, e-commerce, and more.\n",
    "\n",
    "The goal of a decision tree is to split your dataset into groups such that all elements in each group are in similar category or have minimal variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01133f8",
   "metadata": {},
   "source": [
    "<h1> How Decision Trees Work </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ef8c1",
   "metadata": {},
   "source": [
    "There are sevaral variants of the decision tree model. The one we will be considering is the ID3 (Iterative Dichotomiser 3) which is a simple decision tree learning algorithm introduced in 1986 by Quinlan Ross. Lets use an example to build intuion of how it works.\n",
    "\n",
    "Suppose we are trying to predict the gender of a person based off of three features such as hair length, skin color and voice texture. We have the data as shown below\n",
    "\n",
    "<img src='images/dataset.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39391c17",
   "metadata": {},
   "source": [
    "In this scenario, the primary goal of the decision tree is to construct a tree structure where each leaf node represents a group of people with similar genders or minimal disparity in gender. The decision tree aims to create a structure like the one shown below: <img src='images/final_tree.png' />\n",
    "\n",
    "The root node of the decision tree is the \"Voice Texture.\" feature. This feature's values serve as the splitting criterion. Upon splitting the dataset based on the root node (voice texture), we observe that all the individuals on the right side of the tree are females. This indicates a pure node, meaning it requires no further splitting, and we can easily predict \"Female\" for any person whose values align with that branch of the tree.\n",
    "\n",
    "On the left side, we have four males and two females. Unlike the pure node on the right side, this node is impure as it contains a mixture of genders. Hence, we proceed to divide it further by creating a subtree at that impure branch. Each subtree starts with a decision node. In this case, we utilize \"Hair Length\" as the decision node and repeat the same splitting procedure as we did with the root node.\n",
    "\n",
    "Similarly, upon splitting based on hair length, we find that the right branch becomes pure, consisting entirely of males. However, the left branch remains impure with a mix of genders. In this example, I stopped the tree at the second depth and predicted the most common class (modal class). However, we can choose to create a subtree at that branch using \"Complexion\" as the decision node if we desire a deeper tree structure.\n",
    "\n",
    "By constructing this decision tree, we can effectively classify individuals into gender groups based on their voice texture, hair length, and potentially other features if we continue expanding the tree. Decision trees provide a visual and interpretable way to make predictions and understand the decision-making process within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f66ccb",
   "metadata": {},
   "source": [
    "<h1> Some Decision Tree Considerations </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71320246",
   "metadata": {},
   "outputs": [],
   "source": [
    "Looking at the above "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

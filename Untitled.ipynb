{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff8035c",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 30pt\"> DECISION TREES </h1>\n",
    "<img src='images/decision_tree.png' /><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a533e3f1",
   "metadata": {},
   "source": [
    "<h1> What are Decision Trees</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83b030",
   "metadata": {},
   "source": [
    "\n",
    "Decision trees are not your typical linear models. They are powerful and versatile flowchart-like models that can be used for both prediction and classification tasks. While linear regression models assume a linear relationship between predictors and targets, decision trees break free from such constraints. They are non-parametric models, allowing them to capture complex nonlinear relationships between predictors and targets.\n",
    "\n",
    "The beauty of decision trees lies in their ability to handle real-world complexities. They have found immense popularity in various industries, making a significant impact in fields such as healthcare, manufacturing, banking and finance, e-commerce, and more.\n",
    "\n",
    "The goal of a decision tree is to split your dataset into groups such that all elements in each group are in similar category or have minimal variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b48168",
   "metadata": {},
   "source": [
    "<h1> How Decision Trees Work </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afca07f",
   "metadata": {},
   "source": [
    "There are sevaral variants of the decision tree model. The one we will be considering is the ID3 (Iterative Dichotomiser 3) which is a simple decision tree learning algorithm introduced in 1986 by Quinlan Ross. Lets use an example to build intuion of how it works.\n",
    "\n",
    "Suppose we are trying to predict the gender of a person based off of three features such as hair length, skin color and voice texture. We have the data as shown below\n",
    "\n",
    "<img src='images/dataset.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577cb8a6",
   "metadata": {},
   "source": [
    "In this scenario, the primary goal of the decision tree is to construct a tree structure where each leaf node represents a group of people with similar genders or minimal disparity in gender. The decision tree aims to create a structure like the one shown below: <img src='images/final_tree.png' />\n",
    "\n",
    "The root node of the decision tree is the \"Voice Texture.\" feature. This feature's values serve as the splitting criterion. Initially, we have 4 males and 4 females in the entire dataset. Upon splitting the dataset based on the root node (voice texture), we observe that all the individuals on the right branch of the tree are females. This indicates a pure node, meaning it requires no further splitting, and we can easily predict \"Female\" for any person whose values align with that branch of the tree.\n",
    "\n",
    "On the left side, we have four males and two females. Unlike the pure node on the right side, this node is impure as it contains a mixture of genders. Hence, we proceed to divide it further by creating a subtree at that impure branch. Each subtree starts with a decision node. In this case, we utilize \"Hair Length\" as the decision node and repeat the same splitting procedure as we did with the root node.\n",
    "\n",
    "Similarly, upon splitting based on hair length, we find that the right branch becomes pure, consisting entirely of males. However, the left branch remains impure with a mix of genders. In this example, I stopped the tree at the second depth and predicted the most common class (modal class). However, we can choose to create a subtree at that branch using \"Skin color\" as the decision node if we desire a deeper tree structure.\n",
    "\n",
    "Although, we will note that to achieve this tree, there are various aspects and questions we need to put into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc24ed93",
   "metadata": {},
   "source": [
    "<h1> Some Decision Tree Considerations </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa0c08e",
   "metadata": {},
   "source": [
    "\n",
    "### 1. How do we choose the nodes to split by?\n",
    "\n",
    "One of the key challenges in building a decision tree is determining which nodes to split on in order to achieve an optimal measure of purity after each split. While it may be straightforward to manually select features when working with a small dataset, the task becomes more challenging when dealing with a larger number of features, such as 10, 20, 100, or even thousands.\n",
    "\n",
    "In such cases, deciding which feature to split on becomes non-trivial. We need a systematic approach to identify the most informative feature that maximizes the measure of purity after the split. In the ID3 algorithm, each feature is tested for measure of purity. For every feature, the dataset is splitted and the purity is measured. The feature which gives the highest purity is considered for the node. This leads to the next question, how do we measure purity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c291f",
   "metadata": {},
   "source": [
    "### 2. How do we measure purity?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ac156bf",
   "metadata": {},
   "source": [
    "There are various quantitaive measures of purity. The most popular ones being the information gain and the Gini index. We will consider the information gain as our method for this tree. Before we can fully comprehend information gain, lets look at entropy.\n",
    "\n",
    "### Entropy \n",
    "Entropy is a measure of impurity in a group of data. A high entropy indicates a greater degree of uncertainty or disparity and a mixture of different classes or categories, while a low entropy signifies a more homogeneous distribution of classes. Entropy is calculated as \n",
    "<p style='text-align: center; font-size: 20pt'> \n",
    "    $Entropy(D) = -\\sum_{i=1}^{C} p_i \\log_2(p_i)$ <br><br> \n",
    "</p>\n",
    "\n",
    "Using the gender classification example, we can compute impurity before and after spliting by each feature. \n",
    "<img src='images/entropy.png' />\n",
    "\n",
    "<p style=\"font-size: 12pt; font-weight: bold\"> i. Entropy When we split by voice texture as shown in diagram (A)</p> <br> <br>\n",
    "$P(m)$ at the left = <span style='font-size: 12pt'> $\\frac{totalMaleLeft}{totalSamplesLeft} = \\frac{4}{6} = 0.667 $ </span> <br> <br>\n",
    "$P(f)$ at the left = <span style='font-size: 12pt'> $\\frac{totalFemaleLeft}{totalSamplesLeft} = \\frac{1}{6} = 0.1667 $ </span> <br> <br>\n",
    "entropy of left $ H(Left) =  - (P(m) \\times \\log_2(p(m))) + - (P(f) \\times \\log_2(p(f))) = - (0.667 \\times -0.584) + - (0.1667  \\times -2.58467399) = 0.8204$\n",
    "\n",
    "\n",
    "$P(m)$ at the right = <span style='font-size: 12pt'> $\\frac{0}{2} = 0 $ </span> <br> <br>\n",
    "$P(f)$ at the right = <span style='font-size: 12pt'> $\\frac{2}{2} = 1 $ </span> <br> <br>\n",
    "entropy at right <span style='font-size: 12pt'>$ H(Right) =  - (0 \\times \\log_2(0)) + - (1 \\times \\log_2(1)) = - (0) + - (1  \\times 0) = 0 $</span>\n",
    "\n",
    "We can then compute the entropy for this split using voice texture by taking the weighted average of both the left and right entropy. The weights are computed by using the total number of samples in the left divided by the total sample available in that branch as left weight and total number of samples in the right divided by total samples in the branch as right weight.\n",
    "Hence, the weight will be $w_l = \\frac{6}{8} = 0.75$ and $w_r = \\frac{2}{8} = 0.25$\n",
    "\n",
    "Therefore H(P) = $w_l \\times H(Left ) + w_r \\times H(Right) = 0.75 \\times 0.8204 + 0.25 \\times 0 = 0.6153$\n",
    "\n",
    "\n",
    "<p style=\"font-size: 12pt; font-weight: bold\"> i. Entropy When we split by Hair Length as shown in diagram (B)</p> <br> <br>\n",
    "$P(m)$ at the left = <span style='font-size: 12pt'> $\\frac{totalMaleLeft}{totalSamplesLeft} = \\frac{4}{6} = 0.667 $ </span> <br> <br>\n",
    "$P(f)$ at the left = <span style='font-size: 12pt'> $\\frac{totalFemaleLeft}{totalSamplesLeft} = \\frac{1}{6} = 0.1667 $ </span> <br> <br>\n",
    "entropy of left $ H(Left) =  - (P(m) \\times \\log_2(p(m))) + - (P(f) \\times \\log_2(p(f))) = - (0.667 \\times -0.584) + - (0.1667  \\times -2.58467399) = 0.8204$\n",
    "\n",
    "\n",
    "$P(m)$ at the right = <span style='font-size: 12pt'> $\\frac{0}{2} = 0 $ </span> <br> <br>\n",
    "$P(f)$ at the right = <span style='font-size: 12pt'> $\\frac{2}{2} = 1 $ </span> <br> <br>\n",
    "entropy at right <span style='font-size: 12pt'>$ H(Right) =  - (0 \\times \\log_2(0)) + - (1 \\times \\log_2(1)) = - (0) + - (1  \\times 0) = 0 $</span>\n",
    "\n",
    "We can then compute the entropy for this split using voice texture by taking the weighted average of both the left and right entropy. The weights are computed by using the total number of samples in the left divided by the total sample available in that branch as left weight and total number of samples in the right divided by total samples in the branch as right weight.\n",
    "Hence, the weight will be $w_l = \\frac{6}{8} = 0.75$ and $w_r = \\frac{2}{8} = 0.25$\n",
    "\n",
    "Therefore H(P) = $w_l \\times H(Left ) + w_r \\times H(Right) = 0.75 \\times 0.8204 + 0.25 \\times 0 = 0.6153$\n",
    "\n",
    "<b> Information Gain </b><br />\n",
    " Information gain quantifies the reduction in entropy (or uncertainty) achieved by splitting the dataset based on a particular feature. It is calculated by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd9f05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a33b9a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8204"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.3895 + 0.4309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fec266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e601b0b",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 30pt\"> DECISION TREES </h1>\n",
    "<h1 style=\"text-align: center; font-size: 18pt; margin-bottom:50px; font-family:Arial\"> Abubakar Abdulkadir </h1>\n",
    "<img src='images/decision_tree.png' /><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d66741c",
   "metadata": {},
   "source": [
    "<h1> What are Decision Trees</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ebff10",
   "metadata": {},
   "source": [
    "\n",
    "Decision trees are not your typical linear models. They are powerful and versatile flowchart-like models that can be used for both prediction and classification tasks. While linear regression models assume a linear relationship between predictors and targets, decision trees break free from such constraints. They are non-parametric models, allowing them to capture complex nonlinear relationships between predictors and targets.\n",
    "\n",
    "The beauty of decision trees lies in their ability to handle real-world complexities. They have found immense popularity in various industries, making a significant impact in fields such as healthcare, manufacturing, banking and finance, e-commerce, and more.\n",
    "\n",
    "The goal of a decision tree is to split your dataset into groups such that all elements in each group are in similar category or have minimal variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d0eb3c",
   "metadata": {},
   "source": [
    "<h1> How Decision Trees Work </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824f161",
   "metadata": {},
   "source": [
    "There are sevaral variants of the decision tree model. The one we will be considering is the ID3 (Iterative Dichotomiser 3) which is a simple decision tree learning algorithm introduced in 1986 by Quinlan Ross. Lets use an example to build intuion of how it works.\n",
    "\n",
    "Suppose we are trying to predict the gender of a person based off of three features such as hair length, skin color and voice texture. We have the data as shown below\n",
    "\n",
    "<img src='images/dataset.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17be5ad",
   "metadata": {},
   "source": [
    "In this scenario, the primary goal of the decision tree is to construct a tree structure where each leaf node represents a group of people with similar genders or minimal disparity in gender. The decision tree aims to create a structure like the one shown below: <img src='images/final_tree.PNG' />\n",
    "\n",
    "<p style=\"text-align: center\"> Fig 1 </p><br>\n",
    "\n",
    "The root node of the decision tree is the \"Voice Texture.\" feature. This feature's values serve as the splitting criterion. Initially, we have 4 males and 4 females in the entire dataset. Upon splitting the dataset based on the root node (voice texture), we observe that all the individuals on the right branch of the tree are females. This indicates a pure node, meaning it requires no further splitting, and we can easily predict \"Female\" for any person whose values align with that branch of the tree.\n",
    "\n",
    "On the left side, we have four males and two females. Unlike the pure node on the right side, this node is impure as it contains a mixture of genders. Hence, we proceed to divide it further by creating a subtree at that impure branch. Each subtree starts with a decision node. In this case, we utilize \"Hair Length\" as the decision node and repeat the same splitting procedure as we did with the root node.\n",
    "\n",
    "Similarly, upon splitting based on hair length, we find that the right branch becomes pure, consisting entirely of males. However, the left branch remains impure with a mix of genders. In this example, I stopped the tree at the second depth and predicted the most common class (modal class). However, we can choose to create a subtree at that branch using \"Skin color\" as the decision node if we desire a deeper tree structure.\n",
    "\n",
    "Although, we will note that to achieve this tree, there are various aspects and questions we need to put into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d521d0d",
   "metadata": {},
   "source": [
    "<h1> Some Decision Tree Considerations </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b2c0b",
   "metadata": {},
   "source": [
    "\n",
    "### 1. How do we choose the nodes to split by?\n",
    "\n",
    "One of the key challenges in building a decision tree is determining which nodes to split on in order to achieve an optimal measure of purity after each split. While it may be straightforward to manually select features when working with a small dataset, the task becomes more challenging when dealing with a larger number of features, such as 10, 20, 100, or even thousands.\n",
    "\n",
    "In such cases, deciding which feature to split on becomes non-trivial. We need a systematic approach to identify the most informative feature that maximizes the measure of purity after the split. In the ID3 algorithm, each feature is tested for measure of purity. For every feature, the dataset is splitted and the purity is measured. The feature which gives the highest purity is considered for the node. This leads to the next question, how do we measure purity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034e32d",
   "metadata": {},
   "source": [
    "### 2. How do we measure purity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94737eda",
   "metadata": {},
   "source": [
    "There are various quantitaive measures of purity. The most popular ones being the information gain and the Gini index. We will consider the information gain as our method for this tree. Before we can fully comprehend information gain, lets look at entropy.\n",
    "\n",
    "### Entropy \n",
    "Entropy is a measure of impurity in a group of data. A high entropy indicates a greater degree of uncertainty or disparity and a mixture of different classes or categories, while a low entropy signifies a more homogeneous distribution of classes. Entropy is calculated as \n",
    "<p style='text-align: center; font-size: 20pt'> \n",
    "    $Entropy(D) = -\\sum_{i=1}^{C} p_i \\log_2(p_i)$ <br><br> \n",
    "</p>\n",
    "\n",
    "Using the gender classification example, we can compute impurity before and after spliting by each feature. \n",
    "<img src='images/entropy.PNG' />\n",
    "<p style=\"text-align: center\"> Fig 2 </p><br><br>\n",
    "\n",
    "<p style=\"font-size: 12pt; font-weight: bold\"> i. Entropy When we split by voice texture as shown in diagram (A)</p> <br> <br>\n",
    "$P(m)$ at the left = <span style='font-size: 12pt'> $\\frac{totalMaleLeft}{totalSamplesLeft} = \\frac{4}{6} = 0.667 $ </span> <br> <br>\n",
    "$P(f)$ at the left = <span style='font-size: 12pt'> $\\frac{totalFemaleLeft}{totalSamplesLeft} = \\frac{1}{6} = 0.1667 $ </span> <br> <br>\n",
    "entropy of left $ H(Left) =  - (P(m) \\times \\log_2(p(m))) + - (P(f) \\times \\log_2(p(f))) = - (0.667 \\times -0.584) + - (0.1667  \\times -2.58467399) = 0.8204$\n",
    "\n",
    "\n",
    "$P(m)$ at the right = <span style='font-size: 12pt'> $\\frac{0}{2} = 0 $ </span> <br> <br>\n",
    "$P(f)$ at the right = <span style='font-size: 12pt'> $\\frac{2}{2} = 1 $ </span> <br> <br>\n",
    "entropy at right <span style='font-size: 12pt'>$ H(Right) =  - (0 \\times \\log_2(0)) + - (1 \\times \\log_2(1)) = - (0) + - (1  \\times 0) = 0 $</span>\n",
    "\n",
    "We can then compute the entropy for this split using voice texture by taking the weighted average of both the left and right entropy. The weights are computed by using the total number of samples in the left divided by the total sample available in that branch as left weight and total number of samples in the right divided by total samples in the branch as right weight.\n",
    "Hence, the weight will be $w_l = \\frac{6}{8} = 0.75$ and $w_r = \\frac{2}{8} = 0.25$\n",
    "\n",
    "Therefore H(P) = $w_l \\times H(Left ) + w_r \\times H(Right) = 0.75 \\times 0.8204 + 0.25 \\times 0 = 0.6153$\n",
    "\n",
    "\n",
    "<p style=\"font-size: 12pt; font-weight: bold\"> i. Entropy When we split by Hair Length as shown in diagram (B)</p> <br> <br>\n",
    "$P(m)$ at the left = <span style='font-size: 12pt'> $\\frac{totalMaleLeft}{totalSamplesLeft} = \\frac{4}{5} = 0.8 $ </span> <br> <br>\n",
    "$P(f)$ at the left = <span style='font-size: 12pt'> $\\frac{totalFemaleLeft}{totalSamplesLeft} = \\frac{1}{5} = 0.2 $ </span> <br> <br>\n",
    "entropy of left $ H(Left) =  - (P(m) \\times \\log_2(p(m))) + - (P(f) \\times \\log_2(p(f))) = - (0.8 \\times -0.3219) + - (0.2  \\times -2.3219) = 0.7219$\n",
    "\n",
    "\n",
    "$P(m)$ at the right = <span style='font-size: 12pt'> $\\frac{3}{3} = 1 $ </span> <br> <br>\n",
    "$P(f)$ at the right = <span style='font-size: 12pt'> $\\frac{0}{3} = 0 $ </span> <br> <br>\n",
    "entropy at right <span style='font-size: 12pt'>$ H(Right) =  - (1 \\times \\log_2(1)) + - (0 \\times \\log_2(0)) = - (1  \\times 0) + - (0) = 0 $</span>\n",
    "\n",
    "We can then compute the entropy for this split using voice texture by taking the weighted average of both the left and right entropy. The weights are computed by using the total number of samples in the left divided by the total sample available in that branch as left weight and total number of samples in the right divided by total samples in the branch as right weight.\n",
    "Hence, the weight will be $w_l = \\frac{5}{8} = 0.625$ and $w_r = \\frac{3}{8} = 0.375$\n",
    "\n",
    "Therefore H(P) = $w_l \\times H(Left ) + w_r \\times H(Right) = 0.625 \\times 0.7219 + 0.375 \\times 0 = 0.682$\n",
    "\n",
    "### Information Gain \n",
    "Information gain measures the reduction in entropy achieved by splitting the dataset based on a particular feature. The information gain is calculated by comparing the entropy before and after the split. The higher the information gain, the more valuable the feature is in terms of reducing uncertainty and improving the predictive power of the decision tree.\n",
    "\n",
    "To calculate the Information gain for our example A and B. We already calculated the entropy after we split, we will also need to compute the entropy before we split. Then we substract the entropy after split from the entropy before split. The result is our information gain. The feature which gives us the highest information gain will be our feature of choice for spiting at that subset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51929cd5",
   "metadata": {},
   "source": [
    "information gain = entropy before split - entropy after split\n",
    "\n",
    "entropy after split by voice texture (fig A) = 0.6153 <br>\n",
    "entropy after split by Hair Length (fig B) = 0.682\n",
    "\n",
    "To calculate information gain for this two splits A and B we need to compute entropy before split\n",
    "\n",
    "Entropy before split = $-(p(m) \\times \\log_2(p(m))) + -(p(f) \\times \\log_2(p(f)))$ <br><br>\n",
    "$= -(\\frac{4}{8} \\times \\log_2(\\frac{4}{8})) + -(\\frac{4}{8} \\times \\log_2(\\frac{4}{8}))$ <br><br>\n",
    "$= 0.5 + 0.5 = 1 $\n",
    "\n",
    "Before the split, the entropy is at its maximum value (1). This occurs when the dataset contains an equal number of instances from each class. In such cases, the dataset is considered to be the most uncertain or \"unclean\" because neither class dominates the node. Both classes have nearly equal representation, leading to a higher entropy value.\n",
    "\n",
    "A higher entropy indicates a greater level of uncertainty in assigning instances to specific classes at that node. The goal of the decision tree algorithm is to reduce this uncertainty by finding the best split that maximizes the information gain, which is the difference between the entropy before and after the split. By identifying features that can effectively separate the classes and minimize the entropy, the decision tree can make more informed and accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a6abea",
   "metadata": {},
   "source": [
    "<b>information gain when we split by Voice Texture </b> $ = 1 - 0.6153 = $<b style='font-size: 12pt'> 0.3847</b><br><br>\n",
    "<b>information gain when we split by Hair length </b> $ = 1 - 0.682 = $  <b style='font-size: 12pt'> 0.318</b><br><br>\n",
    "\n",
    "Therefore, splitting by voice texture has the highest information gain. Hence, it would be a prefered feature for the split at that node. This is how decision tree algorithm selects suitable feature to slit by at every subtree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d84f4d",
   "metadata": {},
   "source": [
    "### 3. When do we stop creating subtree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dbab10",
   "metadata": {},
   "source": [
    "When constructing a decision tree, it is crucial to determine when to stop creating subtrees. Continuously creating subtrees on a large dataset can lead to overfitting, where the decision tree becomes too specific to the training data and performs poorly on new, unseen data. To find the optimal point to stop splitting, we can use several conditions:\n",
    "\n",
    "1. Pure Node: If all the instances in a node belong to the same class, there is no need to split the node further. This means that the node is already pure, and the decision tree can simply predict that class for all examples that follow that path in the tree. <br><br>\n",
    "\n",
    "2. Specified Number of Depth: We can also stop splitting when the tree has reached a specified maximum depth. For example, if we set a maximum depth of 2, the decision tree will not split further beyond that depth. This prevents the tree from becoming too deep and complex. <br><br>\n",
    "\n",
    "3. Minimum Information Gain: Information gain measures the improvement in predictive power achieved by splitting on a particular attribute. By specifying a minimum information gain or entropy threshold, we can stop splitting when the information gain at a node is less than or equal to that threshold. This prevents the creation of unnecessary splits that do not significantly contribute to improving the predictive ability of the decision tree.<br><br>\n",
    "\n",
    "By utilizing these stopping criteria, we strike a balance between capturing important patterns in the data and avoiding overfitting. The specific conditions chosen for stopping the tree construction depend on the dataset, the problem at hand, and the desired trade-off between complexity and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb470d3",
   "metadata": {},
   "source": [
    "<h1> Pseudocode for Creating a Decision Tree </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898f306",
   "metadata": {},
   "source": [
    "1. Creates a new node.\n",
    "2. Checks if the node meets any stopping criteria, such as purity, maximum depth, or minimum information gain.\n",
    "3. If a stopping criterion is met, returns the node as a leaf node and stops further splitting.\n",
    "4. If no stopping criterion is met, calculates information gain for all features at the current node and selects the highest.\n",
    "5. Splits the dataset at the current node using the selected feature by creating two new nodes: left and right.\n",
    "6. Repeats steps 2 to 6 for each new node created after the split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e93998",
   "metadata": {},
   "source": [
    "<h1 style='background-color: black; color: white; padding: 10px; font-size: 24pt'> Implementing a Decision Tree </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05670961",
   "metadata": {},
   "source": [
    "<h1> From Scratch Using Python </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da28d345",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 30pt\"> INTRODUCTION TO OBJECT DETECTION </h1>\n",
    "<h1 style=\"text-align: center; font-size: 18pt; margin-bottom:50px; font-family:Arial\"> Abubakar Abdulkadir </h1>\n",
    "<img src='images/person.jpg' width='100%' height='200px'/><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4a63b",
   "metadata": {},
   "source": [
    "Object detection is a remarkable computer vision Technique which enables the computer to identify specific objects within an image and to also determine their precise location within the image. \n",
    "\n",
    "Before object detection, Convolutional Neural networks provides prominent methods for classifying images into different classes. We can feed a picture into a Convolutional Neural Network and the network is able to identify if the picture is that of a dog, or a cat or a car etc. This task is called <b> Image Classification.</b> <br><br>\n",
    "\n",
    "<img src=\"images/class.png\" />\n",
    "\n",
    "Image Classification serve as a base for object detection. However, while image classification indeed represented a significant achievement, it had its limitations. Beyond merely knowing the overall content of an image, there is the need to know the precise whereabouts of individual objects within the Image or how many objects are there. This requirement became especially evident in practical applications such as self-driving cars or medical diagnostics where deeper level of understanding was essential to ensure safety and optimal decision-making.\n",
    "\n",
    "<b> Object localization </b> is an approaches which arose to solve the problem of identifying precise location of objects in an image. Object localization deals with predicting bounding boxes that tightly enclose each individual object present in an image. The bounding boxes indicate the position and size of the objects, allowing for their precise location within an image to be indentified. \n",
    "\n",
    "In this approach, the algorithm uses regression technique to determine the coordinates of the box's top-left and bottom-right corners, effectively defining a rectangular region around the detected object. <br><br>\n",
    "\n",
    "<img src=\"images/localization.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b4f429",
   "metadata": {},
   "source": [
    "<b> Object Detection </b> is a combination of image classification and object localization. With these two techniques combined, an objected can be detected within a picture (classification) and also its position predicted through the use of bounding boxes (localization).  <br><br>\n",
    "<img src=\"images/detection.png\" />\n",
    "\n",
    "The object detection approach generally consist of two ideas. The first is detecting all the objects in the image and the second involves, predicting the bounding boxes for each of the detected object. \n",
    "\n",
    "- The Classification\n",
    "\n",
    "    The native multiclass and multilabel Image classification algorithms is limited for object detection use case. The <b>Multiclass image classification </b> classifies an image into one of several classes. It takes a picture and predicts cat or a dog or goat etc. It means that the Multiclass classification is most useful for identifying one object at a time. In object detection, we want to detect deveral objects at a time. \n",
    "\n",
    "    The <b> Multilabel classification </b> detects if one of several objects is present in the image. i.e it can predict 1, 1, 0, 0 indicating that there is a dog, a cat, no goat, no cow in the image. This means that it can detect two or more objects of different kinds but will fail when there are multiple objects of the same kinds in the image. When there are three cows in the image, it can only detect that there is a cow and not three cows. In object detection, we want to detect all the objects in the image whether they are same class or not. <br /><br />\n",
    "    \n",
    "- The regression\n",
    "\n",
    "    In object detection, the task of regression is similar to the native regression problem, where the goal is to predict floating-point numbers. However, in object detection, the regression network needs to predict four floating-point numbers per detected object, representing the bounding box coordinates (top-left x and y, bottom-right x and y) for each object.\n",
    "\n",
    "    Despite the majority of regression use cases outputting a single floating-point number, the regression network can still handle multiple floating-point outputs when trained on target values y of shape (n, 4), where n is the number of training examples, and 4 represents the four bounding box coordinates.\n",
    "    \n",
    "Several methodologies have been proposed over the years beuilding on the combination of object detection and classification. Here are some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb9b9c",
   "metadata": {},
   "source": [
    "## Sliding Windows Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3c2b3",
   "metadata": {},
   "source": [
    "The major challenge in using pure image classification and regression algorithms for object detection is their inability to detect all objects in an image. The Sliding Windows algorithm addresses this by sliding windows of various sizes and spacings across the image, extracting portions of the image to feed into a trained classification network. The trained classification network can then detect if there is an object of interest in that portion or not. If the portion contains an object, it is kept else it is discarded. In the illustrative images bellow, yellow windows indicate detection of objects and the red ones are windows which are negative. In image (b), the windows which do not detect any object have been discarded. We still have several windows detecting the same object. <br /><br />\n",
    "\n",
    "<div style='display: inline-block; width: 40%'><img src='images/slide.png' width='100%'/></div>\n",
    "<div style='display: inline-block;  width: 40%; margin-left: 150px;'><img src='images/slide2.png' width='100%'/></div>\n",
    "\n",
    "The technique known as <b>Non-maximum Suppression (NMS)</b> is used to select the best-fitting bounding box among all the boxes that detect the same object. NMS helps eliminate duplicate detections by considering the predicted bounding boxes' intersection over union (IoU) with the ground-truth bounding box. \n",
    "\n",
    "<p style='font-size: 14pt; text-align: center'> IoU = $\\frac{Area \\ of \\ Intersection}{Area \\ of \\ Union} = \\frac{BB_{\\text{overlap}}}{BB_{\\text{union}}}$ </p>\n",
    "\n",
    "The predicted box with the highest IoU is kept as the best-fitting box, while other overlapping boxes are suppressed to avoid redundant detections. This ensures that each object is only detected once, leading to more accurate and efficient object detection results. \n",
    "\n",
    "The challenge with the Sliding Windows algorithm lies in its computational efficiency, particularly when dealing with larger images or numerous objects. As the algorithm slides the window across the image and extracts portions of it at each position, it needs to classify each of these extracted regions to identify objects and their bounding boxes. The process of classifying every portion of the image can be computationally expensive, especially if the image is large or if there are many objects to detect. Moreover, the sliding window needs to be moved in small increments to ensure that no objects are missed, leading to redundant computations as overlapping regions may be processed multiple times. \n",
    "\n",
    "As a result of these repetitive computations and the need to process numerous regions, the Sliding Windows algorithm can become slow and inefficient, making it less practical for real-time applications or scenarios with stringent speed requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9564327f",
   "metadata": {},
   "source": [
    "# R-CNN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f8e83",
   "metadata": {},
   "source": [
    "The sliding windows algorithm conducts classification and regression for every window in the image, regardless of whether the window contains any relevant features. On the other hand, in the Region Convolutional Neural Network, selective search is applied to the image. This yields around 2000 region proposals which is way smaller than what the number of windows which would have been generated using sliding windows. Consequently, rather than processing all image subsections, attention is focused on these proposed regions and we can run our classification and regression algorithms on these subsections.\n",
    "\n",
    "<img src='images/rcnn.PNG' width='100%'/>\n",
    "\n",
    "The RCNN Algorithm generally consists of three modules - The feature proposal, the feature extraction and the region classification.\n",
    "\n",
    "- <b> Region Proposals: </b>\n",
    "\n",
    "    The object detection algorithm begins by taking an input picture and generating approximately 2000 region proposals using the bottom-up selective search technique. This approach uses the pixel intensities of the image to perform segmentation, dividing the picture into different segments. These segments are added to the list of region proposals, and the process is repeated iteratively. During each repetition, larger segments are generated, making it a \"bottom-up\" process. This means that it starts with smaller segments and progressively creates larger ones until reaching the desired number of around 2000 region proposals. The result is a diverse set of potential object regions that the algorithm will further evaluate for object detection.\n",
    "    \n",
    "<img src='images/regions.jpg' width='50%'/>\n",
    "\n",
    "- <b> Feature Extraction: </b>\n",
    "The next step in the RCNN is the feature extraction which involves extracting fixed size features vector of 4096-dimension from each region proposal. These features are important in classifying the regions as one of our interest objects. <br><br>\n",
    "\n",
    "- <b>Region Classification:</b>\n",
    "    In RCNN, the process of region classification involves using Support Vector Machines (SVMs) trained to detect each object of interest to classify the generated regions. For instance, if the objects of interest are cars and people in an image, each region is separately passed through the car detector SVM. If the region contains a car, the SVM returns a positive detection; otherwise, it returns a negative detection. Similarly, the region is passed through the Person Detector SVM to determine if it contains a person.\n",
    "    Since the same object might be detected multiple times in different regions, a non-maximum suppression (NMS) technique is employed to discard duplicate detections. This ensures that each object is only detected once, improving the accuracy and efficiency of the object detection process. By using SVMs to classify regions and applying NMS to handle duplicate detections, the RCNN algorithm achieves robust and accurate object detection across various objects of interest within the input image.\n",
    "    \n",
    "The challenge with the R-CNN algorithm is the speed and the memory requirement. The algorithm is quite slow because it employs selective search to extract region proposals and then applies separate CNNs on each region. It is also memory consuming due to the storage of the proposed regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d2a6a",
   "metadata": {},
   "source": [
    "# Fast R-CNN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2f1b1",
   "metadata": {},
   "source": [
    "Fast R-CNN builds upon the original R-CNN approach by introducing notable improvements.  Instead of employing selective search to make region proposals, Fast R-CNN extracts feature maps from the image using a single CNN. These feature maps are then utilized for region proposals by employing a technique known as Region of Interest (RoI) pooling. RoI pooling aligns the proposed regions to a fixed size, ensuring efficient processing and enhancing the overall object detection process.\n",
    "\n",
    "<img src='images/fast_rcnn.PNG' width='100%' />\n",
    "\n",
    "Fast R-CNN employs several in novations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9× faster than R-CNN, is 213× faster at test-time, and achieves a higher mAP on PASCAL VOC 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0c68ee",
   "metadata": {},
   "source": [
    "# YOLO Algorithm (You Only Look Once)\n",
    "\n",
    "Several variants of object detection algorithms have been developed over the years with varying performances. Although one which stands out and is currently a favourite due to its significant difference in speed is the YOLO algorithm. \n",
    "\n",
    "The difference between this algorithm and earlier variants is that, this algorithm does not involve multiple stages of classification and prediction from several loops on region proposals. It predicts the objects present in the image and their bounding boxes all in one pass over a convolutional neural network. How is this even possible? Dont we need to identify every item at every position in the picture? How then does YOLO pick out all the objects in the picture without several region proposals? \n",
    "\n",
    "Although, YOLO does not make use of region proposals, it uses a concept similar. Instead of region proposals, YOLO conceptually divides the input image into fixed-sized grids. For an instance, YOLO version 1 uses a 7 x 7 grid. Each grid is responsible for making one prediction, something similar to the region proposals. Except that, unlike the region proposals where we need to extract features and classiify them with different networks, YOLO does both the feature extraction, object classifcation and bounding box prediction all with one network. \n",
    "\n",
    "The YOLO model reduces object detection problem into a regression problem. The network takes an input image of shape 448 * 448. It passes the image through a convolutional neural network extracting features of shape 7 x 7. Each extracted feature represents a grid. The 7 x 7 features are passed into a dense layer and all the image class predictions and the bounding boxes are predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3b095",
   "metadata": {},
   "source": [
    "<h3> THE YOLO ARCHITECTURE </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1db642",
   "metadata": {},
   "source": [
    "<img src='images/yolo_arch.PNG' width='100%' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9004fb0",
   "metadata": {},
   "source": [
    "The Yolo v1 architecture consists of 24 convolutional layers with two fully connected layers. The input of the conv layers is an image of size 448 x 448 which gets reduced to 7 x 7 feature space. The 7 x 7 feature space represents the grid from which predictions are made. The 7 x 7 features are then fed into fully connected layers to regress them into a output of our desired shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d1470b",
   "metadata": {},
   "source": [
    "<h3> ANCHOR BOXES </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715e70c",
   "metadata": {},
   "source": [
    "In the case of a grid-based approach like YOLO, where each grid makes a single prediction, a grid of size 7 x 7 would allow for a maximum of 49 objects to be detected in total. However, a potential challenge arises when a grid contains more than one object. This is where anchor boxes come into play.\n",
    "\n",
    "Anchor boxes are pre-defined boxes of various widths and heights, chosen based on the characteristics of the entire dataset. Let's consider a dataset for detecting people and cars. We can anticipate that cars typically have wider bounding boxes compared to people, while people tend to have taller boxes. To address this, we can define two anchor boxes – one wider and the other taller.\n",
    "\n",
    "By using two anchor boxes, each grid now gains the capability to detect both persons and cars simultaneously, rather than being limited to detecting just one type of object. As a result, when employing two anchor boxes, the model can detect a maximum of 98 objects in total (49 grids x 2 anchor boxes).\n",
    "\n",
    "Selecting the most suitable anchor boxes for our dataset requires a method like clustering. With clustering algorithms, we group the width and height values of all bounding boxes in our dataset into desired clusters. Each cluster represents the characteristics of a specific object category, such as people or cars. The center point of each cluster then becomes the anchor box for that category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8cfd2",
   "metadata": {},
   "source": [
    "<h3> PREPARING DATASET FOR TRAINING A YOLO MODEL </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c225a",
   "metadata": {},
   "source": [
    "To train a YOLO model effectively, we require a carefully curated set of images along with their corresponding annotations. These annotations are vital as they provide the ground truth information necessary for the model to learn object detection accurately.\n",
    "\n",
    "There are several different kinds of annotation formats available, each with its advantages and use cases. Some of the commonly used annotation formats include:\n",
    "\n",
    "1. **YOLO Darknet Format**: This is the native annotation format used by the Darknet framework, which is the original implementation of YOLO. Each annotation file contains information about the object class, bounding box coordinates (center x, center y, width, height), and is typically stored in a .txt file with the same name as the image file.\n",
    "\n",
    "2. **Pascal VOC Format**: The PASCAL Visual Object Classes (VOC) format is widely used in the computer vision community. It utilizes XML files to store annotations, including object class labels, bounding box coordinates, and image-specific details such as segmentation masks and part annotations.\n",
    "\n",
    "3. **COCO Format**: Common Objects in Context (COCO) format is another popular annotation format that uses JSON files to store annotations. It includes information about object categories, bounding box coordinates, segmentation masks, and keypoint annotations.\n",
    "\n",
    "4. **TFRecord Format**: TensorFlow Record (TFRecord) is a binary storage format used by TensorFlow for efficient data storage and loading during training. It allows for storing images and their corresponding annotations in a compact and optimized manner.\n",
    "\n",
    "5. **CSV (Comma-Separated Values) Format**: CSV files are a simple text-based format that can be used for annotation, where each row represents an image and its associated object annotations, typically represented as class labels, bounding box coordinates, and additional attributes.\n",
    "\n",
    "6. **KITTI Format**: The KITTI dataset format is widely used for autonomous driving tasks. It includes text files with object class labels and 2D/3D bounding box coordinates, which are used for training object detection and tracking models.\n",
    "\n",
    "Irrespective of the dataset format we have access to, we can preprocess them to be usefull for YOLO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36aeae8",
   "metadata": {},
   "source": [
    "To preprocess the dataset for training within the YOLO framework, it is crucial to ensure that each image has a corresponding ground truth label. The YOLO model divides each image into an n x n grid (e.g., 7 x 7 for YOLO V1), and each grid generates b (bounding boxes) number of predictions.\n",
    "\n",
    "The training label for each image will have the shape (n, n, b, predictions), where \"predictions\" are represented in the format [P, Cx, Cy, w, h, c1, c2, ..., cn], where:\n",
    "\n",
    "- P is the Confidence score that indicates whether an object is present within the grid. It is 1 if there is an object and 0 otherwise.\n",
    "- Cx is the x-coordinate of the bounding box center relative to the grid cell.\n",
    "- Cy is the y-coordinate of the bounding box center relative to the grid cell.\n",
    "- w is the width of the box relative to the entire image.\n",
    "- h is the height of the box relative to the entire image.\n",
    "- c1, c2, ..., cn represent the classes of objects we intend to detect. For example, if we are detecting persons and cars, the classes will be represented as C1 and C2. The value of C1 will be 1 if the class C1 is detected, and 0 otherwise.\n",
    "\n",
    "For an instance, the prediction  for a detected person in an image can be in this format \n",
    "<img src='images/pred.PNG' width='70%'/>\n",
    "\n",
    "Therefore, for a 7 x 7 grid, 2 anchor boxes and 2 classes of object. The groundtruth label will be of the following shape (7, 7, 2, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8b0170",
   "metadata": {},
   "source": [
    "<p style='text-align: center; font-weight: bolder; font-size: 16pt'> A seperate article will be dedicated to preparing ground truth labels for training a YOLO model. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ea7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
